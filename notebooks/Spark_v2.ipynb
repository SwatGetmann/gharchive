{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b470d347-fa79-46fd-afcf-3ef4771bb434",
   "metadata": {},
   "source": [
    "# Spark, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24002861-28b0-4605-af6c-3497e45b52a6",
   "metadata": {},
   "source": [
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470c0c4-bfd6-4805-bdcf-19bb2cddb133",
   "metadata": {},
   "source": [
    "### IMPORTANT NOTE!\n",
    "\n",
    "Due to very ppor machine I have, I have to read & process HOURLY packages instead of running spark for the whole day of even bigger period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa4132-aaf0-4bb6-b37b-d94401ef3c26",
   "metadata": {},
   "source": [
    "### Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428f1098-1423-4e7a-b6d3-d375e53f9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "\n",
    "spark_resources_path = \"/home/jovyan/work/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770712e1-6bc6-4f77-8330-0bbbf700959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_resources_path_obj = pathlib.Path(spark_resources_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76448ed8-fc10-4ec6-a9c1-b77237b8d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe6240c-3de9-4740-844c-5ab894d1f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"spark://spark:7077\"\n",
    "conf = SparkConf().setAppName(\"Spark TEST v2\").setMaster(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7c32a3-5856-48b1-8402-6e0146a290b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/resources/jars/clickhouse-jdbc-0.4.6-all.jar:/opt/spark/resources/jars/clickhouse-spark-runtime-3.3_2.12-0.7.2.jar'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jars_to_load = [\n",
    "#     \"clickhouse-jdbc-0.4.6-all.jar\",\n",
    "#     \"clickhouse-spark-runtime-3.3_2.12-0.7.2.jar\"\n",
    "# ]\n",
    "\n",
    "# jar_path_joined = \":\".join([\"/opt/spark/resources/jars/{}\".format(jar_name) for jar_name in jars_to_load])\n",
    "# jar_path_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "057605bb-f725-4fce-b768-27de9ee66103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "#   '--jars {} pyspark-shell'.format(\":\".join([\"../jars/{}\".format(jar_name) for jar_name in jars_to_load]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f84881-bdc9-42a4-ad4d-7686400ad2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf.set('spark.jars', jar_path_joined)\n",
    "# conf.set('spark.driver.extraClassPath', jar_path_joined)\n",
    "# conf.set('spark.driver.extraLibraryPath', jar_path_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670ebfb7-4a8a-4769-98d0-384f3e64dba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'Spark TEST v2'), ('spark.master', 'spark://spark:7077')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7340734-45cd-4c22-b1f6-00b60c9d0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=SparkConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907d09b8-0fd5-424a-b348-4474e00e995b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8c69bff5f58f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f21d2f-5c33-4dc6-ad1b-b3d8ce3c3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_date = \"2023-07-23\"\n",
    "glob_pattern = \"{}-*.json.gz\".format(given_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e90d187b-f2f9-48b6-a944-8fac18578e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data/2023-07-23-1.json.gz\n"
     ]
    }
   ],
   "source": [
    "paths = (str(path) for path in sorted(spark_resources_path_obj.glob(glob_pattern), key=os.path.getmtime))\n",
    "cur_path = next(paths)\n",
    "print(cur_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92e63ff-fbc3-46ea-8d9e-db0afc229760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(cur_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d583d642-e714-4dfb-a628-71916d3a4df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112858"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7018d9ba-a9d1-4675-a3e7-a03c547abe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d4487f-96c4-466c-8817-810cc129a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     min(created_at)|     max(created_at)|\n",
      "+--------------------+--------------------+\n",
      "|2023-07-23T01:00:00Z|2023-07-23T01:59:59Z|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT min(created_at), max(created_at) FROM activity\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6687a3ae-9889-4b21-aee8-ccd179d23404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                type|count(1)|\n",
      "+--------------------+--------+\n",
      "|           PushEvent|   80900|\n",
      "|         CreateEvent|   10209|\n",
      "|    PullRequestEvent|    5721|\n",
      "|          WatchEvent|    4711|\n",
      "|   IssueCommentEvent|    3056|\n",
      "|         DeleteEvent|    2236|\n",
      "|         IssuesEvent|    1962|\n",
      "|           ForkEvent|    1106|\n",
      "|PullRequestReview...|     810|\n",
      "|        ReleaseEvent|     767|\n",
      "|  CommitCommentEvent|     537|\n",
      "|PullRequestReview...|     418|\n",
      "|         PublicEvent|     226|\n",
      "|         MemberEvent|     103|\n",
      "|         GollumEvent|      96|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT type, count(*) FROM activity group by type order by count(*) desc\"\n",
    ").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ce70b-1458-4170-b4ea-0373c6117344",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ac49f-480f-4551-a63d-e7c2070df511",
   "metadata": {},
   "source": [
    "* List of Developers that own more than one repository;\n",
    "* List of Developers who did more than one commit in a day, ordered by name and number of commits;\n",
    "* List of Developers with less than one commit in a day;\n",
    "* Total Developers grouped by gender;\n",
    "* Total projects with more than 10 members;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bafa75-e061-4e65-a30a-978df402df77",
   "metadata": {},
   "source": [
    "### LoD - Own > 1 Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5a9103-4a3c-454d-a520-ed24a8b0319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = pyspark.sql.functions.split(df['repo.name'], '/')\n",
    "\n",
    "df = df.withColumn('repo_author', split_col.getItem(0))\n",
    "df = df.withColumn('repo_name', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bef3db2-1f43-474b-ac40-bc093570fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"activity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b4a65-ac96-4dc3-a2fe-ac5e47c461d9",
   "metadata": {},
   "source": [
    "Provide next list of __DISTINCT__ records:\n",
    "```\n",
    "| date | hour | repo author | repo_name |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d84740bb-9b58-4f87-a8a3-d01a9598b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repos = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        date(created_at) as date,\n",
    "        hour(created_at) as hour,\n",
    "        repo_author, \n",
    "        repo_name,\n",
    "        count(*) as total_events\n",
    "    FROM activity \n",
    "    GROUP BY \n",
    "        date,\n",
    "        hour,\n",
    "        repo_author,\n",
    "        repo_name\n",
    "    ORDER BY total_events DESC\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014c384a-2227-4d60-8673-085c139af08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------------+--------------------+------------+\n",
      "|      date|hour|      repo_author|           repo_name|total_events|\n",
      "+----------+----+-----------------+--------------------+------------+\n",
      "|2023-07-23|   1|       1Panel-dev|            appstore|         865|\n",
      "|2023-07-23|   1|    happyfish2024|                mins|         717|\n",
      "|2023-07-23|   1|        B74LABgit|                 CAM|         692|\n",
      "|2023-07-23|   1|          unifyai|                 ivy|         542|\n",
      "|2023-07-23|   1|  bullet-dev-team|   demo-app-env-list|         509|\n",
      "|2023-07-23|   1|  bullet-dev-team|python-pyramid-pu...|         493|\n",
      "|2023-07-23|   1|       CyberCommy|biqu520net-60001-...|         373|\n",
      "|2023-07-23|   1|      timisalin01|               green|         357|\n",
      "|2023-07-23|   1|       CyberCommy|biqu520net-50001-...|         267|\n",
      "|2023-07-23|   1|  networkoperator|demo-cluster-mani...|         265|\n",
      "|2023-07-23|   1|           zomeru|              zomeru|         242|\n",
      "|2023-07-23|   1|       hotspotlab|              hourly|         238|\n",
      "|2023-07-23|   1|black-hat-pikachu|               envoy|         226|\n",
      "|2023-07-23|   1|      exorde-labs|     TestnetProtocol|         224|\n",
      "|2023-07-23|   1|        TalibAzir|       AllahPadishah|         192|\n",
      "|2023-07-23|   1|         zirklbch|  zirklbch.github.io|         178|\n",
      "|2023-07-23|   1|  t56GHBuytH876GH|     t56GHBuytH876GH|         175|\n",
      "|2023-07-23|   1|           okxlin|            appstore|         174|\n",
      "|2023-07-23|   1|      vpnsuperapp|                fast|         168|\n",
      "|2023-07-23|   1|            akum2|               akum2|         164|\n",
      "+----------+----+-----------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_repos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71c974b6-3374-4c2a-9944-60cf63ff76be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Degree of ~compression~ via aggregation: 1.84 (61477 / 112858)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\"\"\"\n",
    "Degree of ~compression~ via aggregation: {:0.2f} ({} / {})\n",
    "\"\"\".format(df.count() / df_repos.count(), df_repos.count(), df.count())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52ef1-757b-430b-b32f-6d719edfb403",
   "metadata": {},
   "source": [
    "#### Limitations & Problems\n",
    "\n",
    "1. As the snapshots in GHARCHIVE are hourly-based, there is __no data__ about the past activity for the repositories. \\\n",
    "So without the past activity, the number of owned repos is incomplete and can be based only on the activities, that we've crawled & stored. \\\n",
    "On how to overcome this, please read the next section, `How can it be improved?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8896f-f095-47db-8d85-d2c350bd6901",
   "metadata": {},
   "source": [
    "#### How can it be improved?\n",
    "\n",
    "* Get bigger slice of data - and do the next thing:\n",
    "  * Store daily snapshots of all unique combinations of `date`, `hour`, `repo.author`, `repo.name`\n",
    "  * Provide the metrics by getting the global `count(distinct repo.name)` aggregation on group of `repo.author`-s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4f9eb-a58f-478b-bc9b-69e8117b4bcf",
   "metadata": {},
   "source": [
    "#### Loading - Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acd0d175-d565-44ce-b132-a36b5a692e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "\n",
    "client = clickhouse_connect.get_client(\n",
    "    host='clickhouse_server', \n",
    "    username='altenar', \n",
    "    password='altenar_ch_demo_517'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a786318f-94a2-459b-a519-b51f39427887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.command('USE gharchive;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24e41de0-542a-4537-a1a8-1bfcf20e3b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.command(\"show tables;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddf5fd79-7c65-4b1e-a1d7-2fb553c6fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE IF NOT EXISTS gharchive.repo_aggregated\n",
      "(\n",
      "    date Date,\n",
      "    hour UInt8,\n",
      "    repo_author String,\n",
      "    repo_name String,\n",
      "    total_events UInt32\n",
      ")\n",
      "ENGINE MergeTree\n",
      "ORDER BY date;\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_cmd = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gharchive.repo_aggregated\n",
    "(\n",
    "    date Date,\n",
    "    hour UInt8,\n",
    "    repo_author String,\n",
    "    repo_name String,\n",
    "    total_events UInt32\n",
    ")\n",
    "ENGINE MergeTree\n",
    "ORDER BY date;\n",
    "\"\"\"\n",
    "\n",
    "print(create_table_cmd)\n",
    "\n",
    "client.command(create_table_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bf785-60c2-4fbd-9185-862d5080b171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78e3c376-8c9c-44d2-b7b5-91e4cfab7644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8c69bff5f58f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.addFile(\"../jars/clickhouse-jdbc-0.4.6-all.jar\")\n",
    "\n",
    "spark.sparkContext._jsc.addJar(\"../jars/clickhouse-jdbc-0.4.6-all.jar\")\n",
    "\n",
    "spark.sparkContext.addFile(\"../jars/clickhouse-spark-runtime-3.3_2.12-0.7.2.jar\")\n",
    "\n",
    "spark.sparkContext._jsc.addJar(\"../jars/clickhouse-spark-runtime-3.3_2.12-0.7.2.jar\")\n",
    "\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a465895-cbd3-4dcc-92ae-dd21581fcd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0322cf13-82ab-49d7-a418-65d7249be76d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o80.save.\n: java.lang.ClassNotFoundException: com.github.housepower.jdbc.ClickHouseDriver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mdf_repos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.github.housepower.jdbc.ClickHouseDriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:clickhouse://clickhouse-server:9000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maltenar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maltenar_ch_demo_517\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgharchive.repo_aggregated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatchsize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43misolationLevel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNONE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1396\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o80.save.\n: java.lang.ClassNotFoundException: com.github.housepower.jdbc.ClickHouseDriver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "df_repos.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"driver\", \"com.github.housepower.jdbc.ClickHouseDriver\") \\\n",
    "    .option(\"url\", \"jdbc:clickhouse://clickhouse-server:9000\") \\\n",
    "    .option(\"user\", \"altenar\") \\\n",
    "    .option(\"password\", \"altenar_ch_demo_517\") \\\n",
    "    .option(\"dbtable\", \"gharchive.repo_aggregated\") \\\n",
    "    .option(\"truncate\", \"true\") \\\n",
    "    .option(\"batchsize\", 10000) \\\n",
    "    .option(\"isolationLevel\", \"NONE\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9831c-1b6d-4626-bd75-4f458753fe51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d038c4-4fe1-4122-9d0b-8c0c64acb058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d502797-3631-49b3-ba3d-65e70bc2423e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5307fbf6-060d-4178-9974-8550ba9c154e",
   "metadata": {},
   "source": [
    "### LoD - Did > 1 Commit a day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c08268-23ac-46b3-8720-29fb61b3a294",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/64605066/explode-array-with-nested-array-raw-spark-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eaa9a6-cd50-4cc6-93a4-54d50022b8d4",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "1. Commits are present inside `payload.commits.author.name`.\\\n",
    "Given the hourly and typized nature of the dataset, we can:\n",
    "    * Filter by `type`, leaving only `PushEvent`-s\n",
    "    * Explode author names\n",
    "    * Get the count aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be832d-8fb2-4188-ac08-708429bd28c6",
   "metadata": {},
   "source": [
    "#### Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2d009ea-16ce-427c-a005-19dc69647649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "|                name|                 ref|  ref_type|              review|                type|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "|                null|                null|      null|                null|          WatchEvent|\n",
      "|          [moyan222]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|         [tokenhash]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|       [TheCamBloch]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|             [Peter]|     refs/heads/safe|      null|                null|           PushEvent|\n",
      "|       [wendellopes]|refs/heads/featur...|      null|                null|           PushEvent|\n",
      "|                null|     lib-default-use|       tag|                null|         CreateEvent|\n",
      "|[DEV Registry Ser...|   refs/heads/master|      null|                null|           PushEvent|\n",
      "|                null|                null|repository|                null|         CreateEvent|\n",
      "|                null|                null|repository|                null|         CreateEvent|\n",
      "|                null|                null|repository|                null|         CreateEvent|\n",
      "|                null|                null|      null|                null|          WatchEvent|\n",
      "|                null|             patch-1|    branch|                null|         DeleteEvent|\n",
      "|           [preciad]|   refs/heads/master|      null|                null|           PushEvent|\n",
      "|                null|                null|      null|                null|           ForkEvent|\n",
      "|                null|                null|      null|                null|          WatchEvent|\n",
      "|[github-actions[b...| refs/heads/gh-pages|      null|                null|           PushEvent|\n",
      "|                null|          new-brznch|    branch|                null|         CreateEvent|\n",
      "|                null|                null|      null|                null|   IssueCommentEvent|\n",
      "|                null|                main|    branch|                null|         CreateEvent|\n",
      "|                null|                null|repository|                null|         CreateEvent|\n",
      "|                null|                main|    branch|                null|         CreateEvent|\n",
      "|[github-actions[b...|   refs/heads/output|      null|                null|           PushEvent|\n",
      "|                null|              master|    branch|                null|         CreateEvent|\n",
      "|                null| devops/add-workflow|    branch|                null|         CreateEvent|\n",
      "|     [hunterkleppek]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|       [Meldarkbini]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|           [Rajnish]|   refs/heads/master|      null|                null|           PushEvent|\n",
      "|   [luisj@sciww.com]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|        [Greg Flynn]|   refs/heads/master|      null|                null|           PushEvent|\n",
      "|                null|           sell-item|    branch|                null|         DeleteEvent|\n",
      "|[sean-morris, sea...|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|                null|                null|      null|                null|        ReleaseEvent|\n",
      "|                null|                null|      null|                null|PullRequestReview...|\n",
      "|                null|                null|repository|                null|         CreateEvent|\n",
      "|                null|                main|    branch|                null|         DeleteEvent|\n",
      "|                null|                null|      null|                null|    PullRequestEvent|\n",
      "|               [Yui]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "| [Surenjanath Singh]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|             [JuanM]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|                null|                null|      null|                null|    PullRequestEvent|\n",
      "|   [Eddie Andersson]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|                null|                null|repository|                null|         CreateEvent|\n",
      "|     [anonymous-tld]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|                null|                null|      null|                null|         IssuesEvent|\n",
      "|                null|                null|      null|                null|         IssuesEvent|\n",
      "|          [cypher03]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "|                null|                null|repository|                null|         CreateEvent|\n",
      "|                null|                null|      null|{{{https://github...|PullRequestReview...|\n",
      "|            [ZXC676]|     refs/heads/main|      null|                null|           PushEvent|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    df[\"payload.commits.author.name\"],\n",
    "    df[\"payload.ref\"],\n",
    "    df[\"payload.ref_type\"],\n",
    "    df[\"payload.review\"],\n",
    "    df[\"type\"],\n",
    ").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea245185-6ac0-4e95-b644-8d335bbb7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|created_at_date|         author_name|\n",
      "+---------------+--------------------+\n",
      "|     2023-07-27|            moyan222|\n",
      "|     2023-07-27|           tokenhash|\n",
      "|     2023-07-27|         TheCamBloch|\n",
      "|     2023-07-27|               Peter|\n",
      "|     2023-07-27|         wendellopes|\n",
      "|     2023-07-27|DEV Registry Service|\n",
      "|     2023-07-27|DEV Registry Service|\n",
      "|     2023-07-27|             preciad|\n",
      "|     2023-07-27| github-actions[bot]|\n",
      "|     2023-07-27| github-actions[bot]|\n",
      "+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        date(created_at) as created_at_date,\n",
    "        explode(payload.commits.author.name) as author_name\n",
    "    FROM activity\n",
    "    WHERE \n",
    "        type = 'PushEvent'\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06810fb5-e4df-465e-ab8f-74b7513f04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+\n",
      "|      date|        author_name|total_commits|\n",
      "+----------+-------------------+-------------+\n",
      "|2023-07-28|github-actions[bot]|        64469|\n",
      "|2023-07-27|github-actions[bot]|        35880|\n",
      "|2023-07-27|        Upptime Bot|        27881|\n",
      "|2023-07-28|        Upptime Bot|        13536|\n",
      "|2023-07-27|      renovate[bot]|        10954|\n",
      "|2023-07-27|           sgou1969|         8488|\n",
      "|2023-07-27|    dependabot[bot]|         8071|\n",
      "|2023-07-28|           sgou1969|         6061|\n",
      "|2023-07-28|      renovate[bot]|         5313|\n",
      "|2023-07-27|         readme-bot|         5125|\n",
      "+----------+-------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        sq.created_at_date as date,\n",
    "        sq.author_name, \n",
    "        count(*) total_commits from \n",
    "    (\n",
    "        SELECT\n",
    "            date(created_at) as created_at_date,\n",
    "            explode(payload.commits.author.name) as author_name\n",
    "        FROM activity\n",
    "        WHERE \n",
    "            type = 'PushEvent'\n",
    "    ) sq\n",
    "    GROUP BY \n",
    "        date, \n",
    "        sq.author_name\n",
    "    HAVING total_commits > 1\n",
    "    ORDER BY \n",
    "        total_commits DESC, \n",
    "        author_name DESC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18932501-296e-4b93-be2d-9f0281498319",
   "metadata": {},
   "source": [
    "#### Limitations & Problems\n",
    "\n",
    "1. Potentially, subquery usage in Spark is not a good practice. Yet, it may be used for prototype reasons.\n",
    "2. We have to additionally group by date, because we have to provide DAILY number of commits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e423e-f7a2-4fd9-9edc-aa77c48cb8ed",
   "metadata": {},
   "source": [
    "#### How can it be improved?\n",
    "\n",
    "* Probably, we should get rid of bot entities :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea7c0a-dfa2-4d9e-bfcb-3cf3dc277ff7",
   "metadata": {},
   "source": [
    "### [!!!] LoD - < 1 commit in a day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12672694-c8e5-447a-83e7-dc98bc2b6b5b",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* The trick is that an author can have 1 commit in, let's say, 25h time span, which is more than 1 day. \\\n",
    "So we should regroup data not by a date, but by consecutive dataspansm which are 25h each...\n",
    "* For to test it, we need at least __2 days worth of data__! \\\n",
    "For now I have complications to get this data, so I skip the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029c1f-e6bb-45d1-90ae-cb48ac248536",
   "metadata": {},
   "source": [
    "#### Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41a219e8-f00a-4c5b-9df1-6283cef3ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|      date|         author_name|total_commits|\n",
      "+----------+--------------------+-------------+\n",
      "|2023-07-28|             R2-D2|            1|\n",
      "|2023-07-28|       moneybot |            1|\n",
      "|2023-07-28|  Samrose Ahmed |            1|\n",
      "|2023-07-27| Scribe of the ...|            1|\n",
      "|2023-07-27|                  |            1|\n",
      "|2023-07-28|              |            1|\n",
      "|2023-07-28|              |            1|\n",
      "|2023-07-27|              |            1|\n",
      "|2023-07-28|                |            1|\n",
      "|2023-07-27|              |            1|\n",
      "+----------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        sq.created_at_date as date,\n",
    "        sq.author_name, \n",
    "        count(*) total_commits from \n",
    "    (\n",
    "        SELECT\n",
    "            date(created_at) as created_at_date,\n",
    "            explode(payload.commits.author.name) as author_name\n",
    "        FROM activity\n",
    "        WHERE \n",
    "            type = 'PushEvent'\n",
    "    ) sq\n",
    "    GROUP BY \n",
    "        date, \n",
    "        sq.author_name\n",
    "    HAVING total_commits <= 1\n",
    "    ORDER BY \n",
    "        total_commits DESC, \n",
    "        author_name DESC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b315c5-527f-4371-b8ea-73da9e4b2490",
   "metadata": {},
   "source": [
    "### [!!!] Total Developers grouped by gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91aaa9-7336-466d-bff9-d55cb58cae3a",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* Proabably can be done via `user[profile_pronouns]` , but for EACH unique GitHub User... That's just dumb.\n",
    "* I haven't found such info (sex,gender, pronouns) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48613a12-5f9e-41b2-b4e8-90f0b94ecc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38145860-9e8c-4397-918d-ab40b4138cbb",
   "metadata": {},
   "source": [
    "### Total projects with more than 10 members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776cedc-b8d0-41b1-8642-806a3cf61cd8",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* Where to find members ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108f9cc-94ff-4a38-b5ad-e07afbc779c3",
   "metadata": {},
   "source": [
    "#### IMPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fcf0fc92-eb7e-4417-b58a-dc60974a198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|       type|              member|\n",
      "+-----------+--------------------+\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "|MemberEvent|{https://avatars....|\n",
      "+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        type,\n",
    "        payload.member\n",
    "    FROM activity\n",
    "    WHERE\n",
    "        payload.member is not NULL\n",
    "        \n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4b2533e2-1e64-422d-9833-9d48b4c7e2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------------+----+------+--------------------+--------------------+--------------------+-----------+\n",
      "|              member|       id|              login|type|action|               login|               login|                name|       type|\n",
      "+--------------------+---------+-------------------+----+------+--------------------+--------------------+--------------------+-----------+\n",
      "|{https://avatars....|107618870|            bsozeau|User| added|                null|      noirblancrouge|noirblancrouge/Pa...|MemberEvent|\n",
      "|{https://avatars....|132350039|         RolivhuwaN|User| added|                null|        Spinofficial| Spinofficial/printf|MemberEvent|\n",
      "|{https://avatars....|132908419|        stheeCamile|User| added|                null|              faellm| faellm/calculoRotas|MemberEvent|\n",
      "|{https://avatars....|140736057|       bevin-crypto|User| added|                null|       Programmer231|Programmer231/Roc...|MemberEvent|\n",
      "|{https://avatars....|136852899|    AntonioEstrada0|User| added|                null|              Drefdu|    Drefdu/DentalNew|MemberEvent|\n",
      "|{https://avatars....|125447139|          udeme-goc|User| added|                null|            code-X16|code-X16/simple_s...|MemberEvent|\n",
      "|{https://avatars....|130376768|           MadDog83|User| added|                null|            maxDes23|maxDes23/project-...|MemberEvent|\n",
      "|{https://avatars....|117692619|           FedeSabi|User| added|                null|            Diego582|Diego582/ideas-pi...|MemberEvent|\n",
      "|{https://avatars....| 32850064|     caseylivingood|User| added|      CaravanStudios|        caravansteve|CaravanStudios/dc...|MemberEvent|\n",
      "|{https://avatars....| 67462827|     IbrahimBagalwa|User| added|                null|           devfabien|devfabien/Gym-git...|MemberEvent|\n",
      "|{https://avatars....| 35641351|              gnmyt|User| added|                null|            luca-ght|luca-ght/NametheFlag|MemberEvent|\n",
      "|{https://avatars....| 77671599|               ybf1|User| added|                null|          ilandroxxy|ilandroxxy/for_te...|MemberEvent|\n",
      "|{https://avatars....|134510152|       Ogah-Dominic|User| added|                null|         Ahmeddavids|Ahmeddavids/ChowF...|MemberEvent|\n",
      "|{https://avatars....| 49315386|           WWWarren|User| added|                null|        luke123adams| luke123adams/kudos2|MemberEvent|\n",
      "|{https://avatars....|140654198|      carolmorcisco|User| added|                null|             WebexCC|WebexCC/webexcc.g...|MemberEvent|\n",
      "|{https://avatars....|140634540|            jensorr|User| added|                null|         seyhosseini|seyhosseini/RA4-P...|MemberEvent|\n",
      "|{https://avatars....|108945706|            robbSab|User| added|         herobots-eu|               hrbts|herobots-eu/lut_ros2|MemberEvent|\n",
      "|{https://avatars....|134038076|     FelipeVitorinu|User| added|                null|      projetodumorro|projetodumorro/fr...|MemberEvent|\n",
      "|{https://avatars....|  1807411|            mhmanik|User| added|                null|          MijanurMim|MijanurMim/practi...|MemberEvent|\n",
      "|{https://avatars....|102827521|        UladzislauM|User| added|                null|           Sunagatov|Sunagatov/Online-...|MemberEvent|\n",
      "|{https://avatars....| 59380096|           noumecha|User| added|                null|            simo2013|   simo2013/depotdi2|MemberEvent|\n",
      "|{https://avatars....|126646740|Saurabh-Singh-Bisht|User| added|            acciojob|     acciojob-3[bot]|acciojob/hello-ac...|MemberEvent|\n",
      "|{https://avatars....|139406515|        Rockinkolin|User| added|                null|       UlfatKhalikov|  UlfatKhalikov/repo|MemberEvent|\n",
      "|{https://avatars....|134438420|            hmzsy27|User| added|                null|           aydaakcay|aydaakcay/TestPro...|MemberEvent|\n",
      "|{https://avatars....|130653136|    mervegulkahveci|User| added|                null|           aydaakcay|aydaakcay/TestPro...|MemberEvent|\n",
      "|{https://avatars....|107501786|      davidmuhumuza|User| added|                null|           nakasanje|nakasanje/my_records|MemberEvent|\n",
      "|{https://avatars....|140740824|      UlfatKhalikov|User| added|                null|         Rockinkolin|    Rockinkolin/Test|MemberEvent|\n",
      "|{https://avatars....|140723341|           jbot4400|User| added|                null|           jheddings|     jheddings/wxdat|MemberEvent|\n",
      "|{https://avatars....|140723341|           jbot4400|User| added|                null|           jheddings|   jheddings/pingdat|MemberEvent|\n",
      "|{https://avatars....|140740824|      UlfatKhalikov|User| added|                null|       StepanSekrier|StepanSekrier/inc...|MemberEvent|\n",
      "|{https://avatars....|140723341|           jbot4400|User| added|                null|           jheddings|jheddings/notes2n...|MemberEvent|\n",
      "|{https://avatars....|138398036|           klgs1234|User| added|       4GeeksAcademy|       4GeeksAcademy|4GeeksAcademy/kar...|MemberEvent|\n",
      "|{https://avatars....|140723341|           jbot4400|User| added|                null|           jheddings|    jheddings/juliet|MemberEvent|\n",
      "|{https://avatars....| 32040453|      shudipto-amin|User| added|                null|           mesaqlain|       mesaqlain/ccg|MemberEvent|\n",
      "|{https://avatars....|131667072|           sibell55|User| added|                null|           aydaakcay|aydaakcay/TestPro...|MemberEvent|\n",
      "|{https://avatars....| 58417734|             lexsvd|User| added|                null|             bkoscar|bkoscar/Real-Time...|MemberEvent|\n",
      "|{https://avatars....|134969451|             kj5210|User| added|                null|             Thomshu|Thomshu/TAK-Compu...|MemberEvent|\n",
      "|{https://avatars....| 77246159|   ErickMarroquin21|User| added|                null|          jiefengsun|jiefengsun/UPS-Po...|MemberEvent|\n",
      "|{https://avatars....| 15098808|           jahilton|User| added|      chanzuckerberg|           pablo-gar|chanzuckerberg/ce...|MemberEvent|\n",
      "|{https://avatars....| 76130958|         serhatgktp|User| added|  ibm-skills-network|  ibm-skills-network|ibm-skills-networ...|MemberEvent|\n",
      "|{https://avatars....|100840225|          ILYA-2606|User| added|                null|            ILYA2606|ILYA2606/Darkness...|MemberEvent|\n",
      "|{https://avatars....| 70247202|       Evan-Luo-jpg|User| added|                null|          hkim113005|     hkim113005/rise|MemberEvent|\n",
      "|{https://avatars....|130515840|           omer2613|User| added|                null|           aydaakcay|aydaakcay/TestPro...|MemberEvent|\n",
      "|{https://avatars....|130502398|           ocvillal|User| added|                null|             nmjxblw|nmjxblw/P5-Evolvi...|MemberEvent|\n",
      "|{https://avatars....|  6818023|     librarianrafia|User| added|SouthernMethodist...|SouthernMethodist...|SouthernMethodist...|MemberEvent|\n",
      "|{https://avatars....|128712250|     unaidedelf8777|User| added|                null|       musabgultekin|musabgultekin/fun...|MemberEvent|\n",
      "|{https://avatars....|116919138|     SvetlanaBeynik|User| added|                null|     rebekahcallkacz|rebekahcallkacz/c...|MemberEvent|\n",
      "|{https://avatars....|140734720|           NuNo1215|User| added|                null|           calebck21|calebck21/Gameristic|MemberEvent|\n",
      "|{https://avatars....|130515354|          S3L1MD3R3|User| added|                null|           aydaakcay|aydaakcay/TestPro...|MemberEvent|\n",
      "|{https://avatars....| 97423977|       Sarjan-Patel|User| added|                null|            mmbrezen|mmbrezen/CSC205Final|MemberEvent|\n",
      "+--------------------+---------+-------------------+----+------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    df[\"payload.member\"],\n",
    "    df[\"payload.member.id\"],\n",
    "    df[\"payload.member.login\"],\n",
    "    df[\"payload.member.type\"],\n",
    "    df[\"payload.action\"],\n",
    "    df[\"org.login\"],\n",
    "    df[\"actor.login\"],\n",
    "    df[\"repo.name\"],\n",
    "    df[\"type\"],\n",
    ").filter(df[\"type\"] == 'MemberEvent').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a307acce-56be-4ac9-9468-52a5bd4cfad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                name|total_member_logins|\n",
      "+--------------------+-------------------+\n",
      "|jgranadoscunoc/re...|                 29|\n",
      "|Jucer74/WebDevelo...|                 17|\n",
      "|hdtoledo/nodejs_a...|                 11|\n",
      "|InstrucJavaReclui...|                  9|\n",
      "|ChungLeba/nextjsv...|                  9|\n",
      "|iramgutierrez/433...|                  9|\n",
      "|emrchi/TestNGProj...|                  8|\n",
      "|aydaakcay/TestPro...|                  7|\n",
      "|ufuk-muhsiroglu/T...|                  7|\n",
      "|GoldenMEmre/com.w...|                  7|\n",
      "+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        repo.name,\n",
    "        count(distinct payload.member.login) as total_member_logins\n",
    "    FROM activity\n",
    "    WHERE\n",
    "        type = 'MemberEvent'\n",
    "        and payload.action = 'added'\n",
    "    GROUP BY\n",
    "        repo.name\n",
    "    ORDER BY\n",
    "        total_member_logins DESC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "96392795-70bf-4e24-b132-eff24abc2373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------------------+-------------------+\n",
      "|      date|hour|                name|total_member_logins|\n",
      "+----------+----+--------------------+-------------------+\n",
      "|2023-07-27|  20|jgranadoscunoc/re...|                 21|\n",
      "|2023-07-28|   1|Jucer74/WebDevelo...|                 16|\n",
      "|2023-07-28|   1|InstrucJavaReclui...|                  9|\n",
      "|2023-07-27|  19|hdtoledo/nodejs_a...|                  9|\n",
      "|2023-07-27|  19|emrchi/TestNGProj...|                  7|\n",
      "|2023-07-27|  19|ufuk-muhsiroglu/T...|                  7|\n",
      "|2023-07-27|  18|aydaakcay/TestPro...|                  7|\n",
      "|2023-07-27|  18|GoldenMEmre/com.w...|                  7|\n",
      "|2023-07-27|  19|mehmetfilik/comWo...|                  6|\n",
      "|2023-07-27|  19|FabianoCarneiro/p...|                  6|\n",
      "+----------+----+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        date(created_at) as date,\n",
    "        hour(created_at) as hour,\n",
    "        repo.name,\n",
    "        count(distinct payload.member.login) as total_member_logins\n",
    "    FROM activity\n",
    "    WHERE\n",
    "        type = 'MemberEvent'\n",
    "        and payload.action = 'added'\n",
    "    GROUP BY\n",
    "        date, hour, repo.name\n",
    "    ORDER BY\n",
    "        total_member_logins DESC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38db048e-cb7a-4fcf-83a3-d9dd75046e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------------------+--------------------+---------------+\n",
      "|      date|hour|          created_at|                name|    member_name|\n",
      "+----------+----+--------------------+--------------------+---------------+\n",
      "|2023-07-27|  18|2023-07-27T18:00:15Z|noirblancrouge/Pa...|        bsozeau|\n",
      "|2023-07-27|  18|2023-07-27T18:00:31Z| Spinofficial/printf|     RolivhuwaN|\n",
      "|2023-07-27|  18|2023-07-27T18:00:33Z| faellm/calculoRotas|    stheeCamile|\n",
      "|2023-07-27|  18|2023-07-27T18:00:39Z|Programmer231/Roc...|   bevin-crypto|\n",
      "|2023-07-27|  18|2023-07-27T18:00:41Z|    Drefdu/DentalNew|AntonioEstrada0|\n",
      "|2023-07-27|  18|2023-07-27T18:00:44Z|code-X16/simple_s...|      udeme-goc|\n",
      "|2023-07-27|  18|2023-07-27T18:01:13Z|maxDes23/project-...|       MadDog83|\n",
      "|2023-07-27|  18|2023-07-27T18:01:15Z|Diego582/ideas-pi...|       FedeSabi|\n",
      "|2023-07-27|  18|2023-07-27T18:01:16Z|CaravanStudios/dc...| caseylivingood|\n",
      "|2023-07-27|  18|2023-07-27T18:01:20Z|devfabien/Gym-git...| IbrahimBagalwa|\n",
      "+----------+----+--------------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        date(created_at) as date,\n",
    "        hour(created_at) as hour,\n",
    "        created_at,\n",
    "        repo.name,\n",
    "        payload.member.login as member_name\n",
    "    FROM activity\n",
    "    WHERE\n",
    "        type = 'MemberEvent'\n",
    "        and payload.action = 'added'\n",
    "    ORDER BY\n",
    "        date ASC,\n",
    "        hour ASC,\n",
    "        created_at ASC,\n",
    "        repo.name ASC,\n",
    "        member_name ASC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a35c5a-0d23-4572-b460-6a64a1acec15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Limitations & Problems\n",
    "\n",
    "1. As the snapshots in GHARCHIVE are hourly-based, there is __no data__ about the past activity for the repositories. \\\n",
    "So without the past activity, the number of members is incomplete and can be based only on the activities, like 'MemberEvent', that we've crawled & stored. \\\n",
    "On how to overcome this, please read the next section, `How can it be improved?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce98c5-2b5e-4301-bdaa-de66c582b554",
   "metadata": {},
   "source": [
    "#### How can it be improved?\n",
    "\n",
    "* Get bigger slice of data - and do the next thing:\n",
    "  * Store daily snapshots of all unique combinations of `date`, `repo`, `member`, 'action'\n",
    "  * Provide the metrics by getting the global `count(distinct member)` aggregation on group of `repo`-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44d0ad-ff5a-48dc-ba49-86aef461211d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9677a-3380-42f2-b4af-579b07f186cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea64f0-0248-4800-926a-3760e7cc9bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
